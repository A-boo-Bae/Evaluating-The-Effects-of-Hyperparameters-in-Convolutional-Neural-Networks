This project thesis investigates the impact of hyperparameter variations on Convolutional Neural Network (CNN) performance. Five distinct CNN architectures, namely VGG16, ResNet50, MobileNetV2, DenseNet121, and EfficientNetB0, were evaluated using the CIFAR10 and CIFAR100 datasets. Through rigorous experimentation and analysis, this study examines the sensitivity of each architecture to hyperparameter settings, focusing on parameters like learning rate, batch size, and optimizer choice.
The methodology employed for hyperparameter selection involved two main sampling techniques: Random Sampler and Metaheuristic Sampler. These methods were used to explore a wide range of hyperparameter values for each CNN architecture on the CIFAR10 and CIFAR100 datasets. Following hyperparameter selection, a comprehensive statistical analysis was conducted to uncover correlations between metric outcomes and hyperparameter values. By systematically varying learning rates, batch sizes, and optimizers, this study sheds light on the intricate relationship between hyperparameters and CNN performance for the specified architectures and datasets.
The CIFAR10 and CIFAR100 datasets were analysed using Pearson Correlation and Regression Analysis for various CNN architectures. For CIFAR10, larger batch sizes correlated positively with validation F1-scores, especially in VGG16, DenseNet121, EfficientNetB0, and ResNet50. Dropout rates had varying effects across models, benefiting DenseNet121 but potentially harming MobileNetV2 and ResNet50. The number of epochs had a moderate positive impact, while lower learning rates consistently led to better F1-scores. Optimizer choice showed a positive correlation. For CIFAR100, hyperparameters' relationships with F1-scores varied between models. Dropout rates consistently boosted scores, while batch size, learning rate, optimizer, and epochs had mixed effects depending on architecture.
